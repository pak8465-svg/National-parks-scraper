{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# National Parks Brochure Scraper\n",
    "\n",
    "This notebook scrapes park brochures from the U.S. National Parks Service website, extracts text and metadata, and writes results to Google Sheets.\n",
    "\n",
    "## Features\n",
    "- Scrapes 20 National Park brochures (configurable)\n",
    "- Extracts park name, state, established year, and size\n",
    "- Writes to Google Sheets\n",
    "- Respects 10-second delay between requests\n",
    "- Comprehensive error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required dependencies and import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install PyPDF2 gspread google-auth google-auth-oauthlib google-auth-httplib2 -q\n",
    "\n",
    "print(\"‚úì Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import io\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "import gspread\n",
    "from google.auth import default\n",
    "from google.colab import auth\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auth"
   },
   "source": [
    "## Google Authentication\n",
    "\n",
    "Authenticate with Google to access Google Sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "authenticate"
   },
   "outputs": [],
   "source": [
    "# Authenticate with Google\n",
    "auth.authenticate_user()\n",
    "creds, _ = default()\n",
    "\n",
    "print(\"‚úì Google authentication successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## Configuration\n",
    "\n",
    "Set your Google Sheets URL and scraping parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "configuration"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SPREADSHEET_URL = \"https://docs.google.com/spreadsheets/d/1QfsxIUok_5owSTJvI1_V5GNuzTsAOShHxktxh9w_jHA/edit?usp=sharing\"\n",
    "LIMIT = 20  # Number of parks to scrape\n",
    "DELAY_SECONDS = 10  # Delay between requests\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Spreadsheet: {SPREADSHEET_URL}\")\n",
    "print(f\"  Parks to scrape: {LIMIT}\")\n",
    "print(f\"  Delay: {DELAY_SECONDS} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scraper_class"
   },
   "source": [
    "## Scraper Class Definition\n",
    "\n",
    "Define the main scraper class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scraper"
   },
   "outputs": [],
   "source": [
    "class NationalParksScraper:\n",
    "    \"\"\"Scraper for National Parks brochures\"\"\"\n",
    "\n",
    "    # Major National Parks with their 4-letter codes\n",
    "    PARK_CODES = [\n",
    "        ('yell', 'Yellowstone'),\n",
    "        ('yose', 'Yosemite'),\n",
    "        ('grca', 'Grand Canyon'),\n",
    "        ('zion', 'Zion'),\n",
    "        ('acad', 'Acadia'),\n",
    "        ('glac', 'Glacier'),\n",
    "        ('romo', 'Rocky Mountain'),\n",
    "        ('olym', 'Olympic'),\n",
    "        ('grsm', 'Great Smoky Mountains'),\n",
    "        ('shen', 'Shenandoah'),\n",
    "        ('arch', 'Arches'),\n",
    "        ('cany', 'Canyonlands'),\n",
    "        ('brca', 'Bryce Canyon'),\n",
    "        ('jotr', 'Joshua Tree'),\n",
    "        ('deva', 'Death Valley'),\n",
    "        ('seki', 'Sequoia'),\n",
    "        ('redw', 'Redwood'),\n",
    "        ('noca', 'North Cascades'),\n",
    "        ('mora', 'Mount Rainier'),\n",
    "        ('grte', 'Grand Teton'),\n",
    "        ('badl', 'Badlands'),\n",
    "        ('cave', 'Carlsbad Caverns'),\n",
    "        ('pefo', 'Petrified Forest'),\n",
    "        ('thro', 'Theodore Roosevelt'),\n",
    "        ('meve', 'Mesa Verde'),\n",
    "        ('crla', 'Crater Lake'),\n",
    "        ('lavo', 'Lassen Volcanic'),\n",
    "        ('chis', 'Channel Islands'),\n",
    "        ('pinn', 'Pinnacles'),\n",
    "        ('kova', 'Kobuk Valley'),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, delay_seconds: int = 10):\n",
    "        self.delay_seconds = delay_seconds\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        })\n",
    "        self.results = []\n",
    "\n",
    "    def find_brochure_url(self, park_code: str, park_name: str) -> Optional[str]:\n",
    "        \"\"\"Find the brochure URL for a given park\"\"\"\n",
    "        patterns = [\n",
    "            f\"https://www.nps.gov/{park_code}/planyourvisit/upload/{park_name.replace(' ', '-')}-Brochure.pdf\",\n",
    "            f\"https://www.nps.gov/{park_code}/planyourvisit/upload/{park_name.replace(' ', '-')}-brochure.pdf\",\n",
    "            f\"https://www.nps.gov/{park_code}/learn/upload/{park_name.replace(' ', '-')}-Brochure.pdf\",\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            brochure_page = f\"https://www.nps.gov/{park_code}/planyourvisit/brochures.htm\"\n",
    "            response = self.session.get(brochure_page, timeout=30)\n",
    "            if response.status_code == 200:\n",
    "                pdf_links = re.findall(r'href=\"([^\"]*\\.pdf)\"', response.text)\n",
    "                if pdf_links:\n",
    "                    pdf_url = pdf_links[0]\n",
    "                    if not pdf_url.startswith('http'):\n",
    "                        pdf_url = f\"https://www.nps.gov{pdf_url}\" if pdf_url.startswith('/') else f\"https://www.nps.gov/{park_code}/planyourvisit/{pdf_url}\"\n",
    "                    return pdf_url\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for pattern in patterns:\n",
    "            try:\n",
    "                response = self.session.head(pattern, timeout=10, allow_redirects=True)\n",
    "                if response.status_code == 200:\n",
    "                    return pattern\n",
    "            except:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    def download_pdf(self, url: str) -> Optional[bytes]:\n",
    "        \"\"\"Download PDF from URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_content: bytes) -> str:\n",
    "        \"\"\"Extract text from PDF content\"\"\"\n",
    "        try:\n",
    "            pdf_file = io.BytesIO(pdf_content)\n",
    "            reader = PdfReader(pdf_file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def parse_park_info(self, text: str, park_name: str) -> Dict[str, str]:\n",
    "        \"\"\"Parse park information from extracted text\"\"\"\n",
    "        info = {\n",
    "            'park_name': park_name,\n",
    "            'state': '',\n",
    "            'established_year': '',\n",
    "            'size': ''\n",
    "        }\n",
    "\n",
    "        # Extract state\n",
    "        state_pattern = r'\\b(Alabama|Alaska|Arizona|Arkansas|California|Colorado|Connecticut|Delaware|Florida|Georgia|Hawaii|Idaho|Illinois|Indiana|Iowa|Kansas|Kentucky|Louisiana|Maine|Maryland|Massachusetts|Michigan|Minnesota|Mississippi|Missouri|Montana|Nebraska|Nevada|New Hampshire|New Jersey|New Mexico|New York|North Carolina|North Dakota|Ohio|Oklahoma|Oregon|Pennsylvania|Rhode Island|South Carolina|South Dakota|Tennessee|Texas|Utah|Vermont|Virginia|Washington|West Virginia|Wisconsin|Wyoming)\\b'\n",
    "        state_match = re.search(state_pattern, text, re.IGNORECASE)\n",
    "        if state_match:\n",
    "            info['state'] = state_match.group(1)\n",
    "\n",
    "        # Extract established year\n",
    "        year_patterns = [\n",
    "            r'[Ee]stablished[:\\s]+(?:in\\s+)?(\\d{4})',\n",
    "            r'[Dd]esignated[:\\s]+(?:in\\s+)?(\\d{4})',\n",
    "            r'[Cc]reated[:\\s]+(?:in\\s+)?(\\d{4})',\n",
    "        ]\n",
    "        for pattern in year_patterns:\n",
    "            year_match = re.search(pattern, text)\n",
    "            if year_match:\n",
    "                year = year_match.group(1)\n",
    "                if 1850 <= int(year) <= datetime.now().year:\n",
    "                    info['established_year'] = year\n",
    "                    break\n",
    "\n",
    "        # Extract size\n",
    "        size_patterns = [\n",
    "            r'(\\d+[\\d,]*)\\s+acres',\n",
    "            r'(\\d+[\\d,]*)\\s+square\\s+miles',\n",
    "        ]\n",
    "        for pattern in size_patterns:\n",
    "            size_match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if size_match:\n",
    "                info['size'] = size_match.group(0)\n",
    "                break\n",
    "\n",
    "        return info\n",
    "\n",
    "    def scrape_parks(self, limit: int = 20) -> List[Dict[str, str]]:\n",
    "        \"\"\"Scrape park brochures\"\"\"\n",
    "        print(f\"Starting to scrape up to {limit} parks...\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        count = 0\n",
    "        for park_code, park_name in self.PARK_CODES:\n",
    "            if count >= limit:\n",
    "                break\n",
    "\n",
    "            print(f\"\\n[{count + 1}/{limit}] {park_name}...\")\n",
    "\n",
    "            try:\n",
    "                brochure_url = self.find_brochure_url(park_code, park_name)\n",
    "                if not brochure_url:\n",
    "                    print(f\"  ‚ùå No brochure found\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"  üìÑ {brochure_url}\")\n",
    "\n",
    "                pdf_content = self.download_pdf(brochure_url)\n",
    "                if not pdf_content:\n",
    "                    print(f\"  ‚ùå Download failed\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"  ‚úì Downloaded ({len(pdf_content)} bytes)\")\n",
    "\n",
    "                text = self.extract_text_from_pdf(pdf_content)\n",
    "                if not text:\n",
    "                    print(f\"  ‚ùå Text extraction failed\")\n",
    "                    continue\n",
    "\n",
    "                print(f\"  ‚úì Extracted {len(text)} chars\")\n",
    "\n",
    "                info = self.parse_park_info(text, park_name)\n",
    "                info['brochure_url'] = brochure_url\n",
    "                self.results.append(info)\n",
    "                count += 1\n",
    "\n",
    "                print(f\"  ‚úì State: {info['state']}, Year: {info['established_year']}, Size: {info['size']}\")\n",
    "\n",
    "                if count < limit:\n",
    "                    print(f\"  ‚è≥ Waiting {self.delay_seconds} seconds...\")\n",
    "                    time.sleep(self.delay_seconds)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error: {e}\")\n",
    "                continue\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"‚úì Complete! Processed {len(self.results)} parks\")\n",
    "        return self.results\n",
    "\n",
    "    def write_to_google_sheets(self, spreadsheet_url: str, creds):\n",
    "        \"\"\"Write results to Google Sheets\"\"\"\n",
    "        try:\n",
    "            gc = gspread.authorize(creds)\n",
    "            spreadsheet_id = spreadsheet_url.split('/d/')[1].split('/')[0]\n",
    "            spreadsheet = gc.open_by_key(spreadsheet_id)\n",
    "\n",
    "            try:\n",
    "                worksheet = spreadsheet.worksheet('Park Data')\n",
    "            except:\n",
    "                worksheet = spreadsheet.add_worksheet(title='Park Data', rows=100, cols=10)\n",
    "\n",
    "            headers = ['Park Name', 'State', 'Established Year', 'Size', 'Brochure URL', 'Scraped Date']\n",
    "            data = [headers]\n",
    "\n",
    "            for result in self.results:\n",
    "                row = [\n",
    "                    result['park_name'],\n",
    "                    result['state'],\n",
    "                    result['established_year'],\n",
    "                    result['size'],\n",
    "                    result['brochure_url'],\n",
    "                    datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                ]\n",
    "                data.append(row)\n",
    "\n",
    "            worksheet.clear()\n",
    "            worksheet.update('A1', data)\n",
    "\n",
    "            print(f\"\\n‚úì Wrote {len(self.results)} rows to Google Sheets\")\n",
    "            print(f\"  Sheet: {spreadsheet.title}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error writing to Google Sheets: {e}\")\n",
    "\n",
    "print(\"‚úì Scraper class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "run"
   },
   "source": [
    "## Run the Scraper\n",
    "\n",
    "Execute the scraping process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "execute"
   },
   "outputs": [],
   "source": [
    "# Create and run scraper\n",
    "scraper = NationalParksScraper(delay_seconds=DELAY_SECONDS)\n",
    "results = scraper.scrape_parks(limit=LIMIT)\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Scraped {len(results)} parks successfully!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sheets"
   },
   "source": [
    "## Write to Google Sheets\n",
    "\n",
    "Save the results to your Google Sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write_sheets"
   },
   "outputs": [],
   "source": [
    "# Write to Google Sheets\n",
    "if results:\n",
    "    scraper.write_to_google_sheets(SPREADSHEET_URL, creds)\n",
    "else:\n",
    "    print(\"No results to write to Google Sheets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "preview"
   },
   "source": [
    "## Preview Results\n",
    "\n",
    "Display a preview of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_results"
   },
   "outputs": [],
   "source": [
    "# Display results preview\n",
    "import pandas as pd\n",
    "\n",
    "if results:\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df[['park_name', 'state', 'established_year', 'size']]\n",
    "    print(\"\\nResults Preview:\")\n",
    "    print(df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No results to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save"
   },
   "source": [
    "## Save to JSON (Optional)\n",
    "\n",
    "Save a backup copy as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_json"
   },
   "outputs": [],
   "source": [
    "# Save to JSON\n",
    "if results:\n",
    "    with open('parks_data.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(\"‚úì Results saved to parks_data.json\")\n",
    "    \n",
    "    # Download the file\n",
    "    from google.colab import files\n",
    "    files.download('parks_data.json')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "National Parks Brochure Scraper",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
